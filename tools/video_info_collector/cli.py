#!/usr/bin/env python3
"""
Video Info Collector CLI
ç¬¦åˆè®¾è®¡æ–‡æ¡£çš„ä¸¤é˜¶æ®µå·¥ä½œæµå®ç°
"""

import argparse
import sys
import os
import yaml
import signal
import logging
from datetime import datetime
from pathlib import Path

from .scanner import VideoFileScanner
from .metadata import VideoMetadataExtractor
from .csv_writer import CSVWriter
from .sqlite_storage import SQLiteStorage
from .error_handler import (
    ErrorHandler, 
    create_error_handler,
    VideoInfoCollectorError,
    FileNotFoundError as VICFileNotFoundError,
    PermissionError as VICPermissionError,
    DatabaseError,
    MetadataExtractionError
)

# å…¨å±€å˜é‡
_error_handler = None
_interrupted = False
_current_operation = None


def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
    def signal_handler(signum, frame):
        global _interrupted, _current_operation
        _interrupted = True
        
        print("\n")
        print("ğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)")
        
        if _current_operation:
            print(f"æ­£åœ¨ä¼˜é›…åœ°åœæ­¢å½“å‰æ“ä½œ: {_current_operation}")
        else:
            print("æ­£åœ¨ä¼˜é›…åœ°åœæ­¢ç¨‹åº...")
        
        print("è¯·ç¨ç­‰ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
        
        # ç»™ç¨‹åºä¸€äº›æ—¶é—´æ¥æ¸…ç†èµ„æº
        import time
        time.sleep(0.5)
        
        print("âœ… ç¨‹åºå·²å®‰å…¨é€€å‡º")
        sys.exit(130)  # 130 æ˜¯ SIGINT çš„æ ‡å‡†é€€å‡ºç 
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)


def check_interruption():
    """æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­"""
    global _interrupted
    if _interrupted:
        print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)


def set_current_operation(operation: str):
    """è®¾ç½®å½“å‰æ“ä½œæè¿°"""
    global _current_operation
    _current_operation = operation


def setup_logging(debug_mode: bool = False):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    if debug_mode:
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stderr)
            ]
        )
    else:
        logging.basicConfig(level=logging.WARNING)


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    # ä½¿ç”¨ç¨³å®šçš„è·¯å¾„ç®¡ç†å·¥å…·è·å–é…ç½®æ–‡ä»¶è·¯å¾„
    from ..path_utils import get_config_path
    config_path = get_config_path("tools/video_info_collector/config.yaml", calling_file=__file__)
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤é…ç½®
        return {
            'output_paths': {
                'base_dir': 'output/video_info_collector',
                'csv_dir': 'csv',
                'database_dir': 'database',
                'default_database': 'video_database.db',
                'temp_csv_prefix': 'temp_video_info_'
            }
        }


def get_default_paths():
    """è·å–é»˜è®¤çš„è¾“å‡ºè·¯å¾„"""
    config = load_config()
    output_config = config.get('output_paths', {})
    
    base_dir = output_config.get('base_dir', 'output/video_info_collector')
    csv_dir = output_config.get('csv_dir', 'csv')
    database_dir = output_config.get('database_dir', 'database')
    default_database = output_config.get('default_database', 'video_database.db')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    csv_path = Path(base_dir) / csv_dir
    database_path = Path(base_dir) / database_dir
    csv_path.mkdir(parents=True, exist_ok=True)
    database_path.mkdir(parents=True, exist_ok=True)
    
    return {
        'csv_dir': str(csv_path),
        'database_dir': str(database_path),
        'default_database': str(database_path / default_database),
        'temp_csv_prefix': output_config.get('temp_csv_prefix', 'temp_video_info_')
    }


def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    # å¦‚æœå·²ç»æ˜¯æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
    if isinstance(size_bytes, str):
        return size_bytes
    
    # å¦‚æœæ˜¯Noneæˆ–0ï¼Œè¿”å›N/A
    if not size_bytes:
        return 'N/A'
    
    # è½¬æ¢ä¸ºæ•´æ•°ï¼ˆé˜²æ­¢æµ®ç‚¹æ•°è¾“å…¥ï¼‰
    try:
        size_bytes = int(size_bytes)
    except (ValueError, TypeError):
        return 'N/A'
    
    if size_bytes < 1024:
        return f"{size_bytes}B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.2f}KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes / (1024 * 1024):.2f}MB"
    else:
        return f"{size_bytes / (1024 * 1024 * 1024):.2f}GB"


def format_duration(seconds):
    """æ ¼å¼åŒ–æ—¶é•¿"""
    if seconds is None:
        return "æœªçŸ¥"
    
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    else:
        return f"{minutes:02d}:{secs:02d}"


def generate_directory_based_filename(directory_path, timestamp, prefix="temp_video_info_"):
    """
    åŸºäºç›®å½•è·¯å¾„ç”ŸæˆCSVæ–‡ä»¶åï¼Œå»æ‰volumeså±‚
    
    Args:
        directory_path: æ‰«æçš„ç›®å½•è·¯å¾„
        timestamp: æ—¶é—´æˆ³å­—ç¬¦ä¸²
        prefix: æ–‡ä»¶åå‰ç¼€ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
        
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶å
        
    Examples:
        /Volumes/ws2/media/videos/ -> ws2_media_videos_20241027_181559.csv
        /path/to/videos/Movies/ -> path_to_videos_Movies_20241027_181559.csv
        /media/storage/videos/ -> media_storage_videos_20241027_181559.csv
    """
    path = Path(directory_path).resolve()
    parts = path.parts
    
    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„è·¯å¾„éƒ¨åˆ†
    filtered_parts = []
    for part in parts:
        # è·³è¿‡æ ¹ç›®å½•ã€Volumeså±‚å’Œç©ºå­—ç¬¦ä¸²
        if part in ('/', '', 'Volumes'):
            continue
        # è·³è¿‡ä»¥ç‚¹å¼€å¤´çš„éšè—ç›®å½•
        if part.startswith('.'):
            continue
        filtered_parts.append(part)
    
    if not filtered_parts:
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è·¯å¾„éƒ¨åˆ†ï¼Œä½¿ç”¨é»˜è®¤å‰ç¼€
        return f"{prefix}{timestamp}.csv"
    
    # å°†è·¯å¾„éƒ¨åˆ†è¿æ¥ä¸ºæ–‡ä»¶åï¼Œé™åˆ¶é•¿åº¦é¿å…æ–‡ä»¶åè¿‡é•¿
    max_parts = 4  # æœ€å¤šä½¿ç”¨4ä¸ªè·¯å¾„éƒ¨åˆ†
    if len(filtered_parts) > max_parts:
        filtered_parts = filtered_parts[:max_parts]
    
    # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
    clean_parts = []
    for part in filtered_parts:
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—å’Œä¸­æ–‡å­—ç¬¦
        clean_part = ''.join(c if c.isalnum() or '\u4e00' <= c <= '\u9fff' else '_' for c in part)
        # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
        clean_part = '_'.join(filter(None, clean_part.split('_')))
        if clean_part:
            clean_parts.append(clean_part)
    
    if not clean_parts:
        return f"{prefix}{timestamp}.csv"
    
    filename_base = '_'.join(clean_parts)
    return f"{filename_base}_{timestamp}.csv"


def scan_command(args):
    """æ‰«æç›®å½•å¹¶æ ¹æ®è¾“å‡ºæ ¼å¼ç”Ÿæˆæ–‡ä»¶"""
    global _error_handler
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    set_current_operation("ç›®å½•æ‰«æ")
    
    # éªŒè¯ç›®å½•è·¯å¾„
    if not _error_handler.validate_file_path(args.directory, "ç›®å½•", must_exist=True):
        return 1
    
    directory = Path(args.directory)
    
    # è·å–é»˜è®¤è·¯å¾„é…ç½®
    default_paths = get_default_paths()
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å’Œæ ¼å¼
    output_format = getattr(args, 'output_format', 'csv')
    
    if args.output:
        # ç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶
        output_file = args.output
        # ä»æ–‡ä»¶æ‰©å±•åæ¨æ–­æ ¼å¼
        if output_file.endswith('.db') or output_file.endswith('.sqlite'):
            output_format = 'sqlite'
        elif output_file.endswith('.csv'):
            output_format = 'csv'
    else:
        # ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if output_format == 'sqlite':
            output_filename = f"video_info_{timestamp}.db"
            output_file = str(Path(default_paths['database_dir']) / output_filename)
        else:  # csv æˆ–ä¸´æ—¶æ–‡ä»¶
            if args.temp_file:
                output_file = args.temp_file
            else:
                # ä½¿ç”¨åŸºäºç›®å½•è·¯å¾„çš„æ–‡ä»¶å‘½å
                temp_filename = generate_directory_based_filename(
                    directory, timestamp, default_paths['temp_csv_prefix']
                )
                output_file = str(Path(default_paths['csv_dir']) / temp_filename)
    
    print(f"æ­£åœ¨æ‰«æç›®å½•: {directory}")
    print(f"è¾“å‡ºæ ¼å¼: {output_format}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    if _error_handler.verbose:
        print(f"ğŸ”§ é€’å½’æ‰«æ: {'æ˜¯' if args.recursive else 'å¦'}")
        print(f"ğŸ”§ é¢„è§ˆæ¨¡å¼: {'æ˜¯' if args.dry_run else 'å¦'}")
        if args.tags:
            print(f"ğŸ·ï¸  æ ‡ç­¾: {args.tags}")
        if args.path:
            print(f"ğŸ“‚ é€»è¾‘è·¯å¾„: {args.path}")
        print(f"ğŸ”§ æ–‡ä»¶æ‰©å±•å: {args.extensions}")
    print()
    
    try:
        # åˆå§‹åŒ–æ‰«æå™¨å’Œå…ƒæ•°æ®æå–å™¨
        set_current_operation("åˆå§‹åŒ–æ‰«æå™¨")
        scanner = VideoFileScanner()
        metadata_extractor = VideoMetadataExtractor()
        
        # æ‰«æè§†é¢‘æ–‡ä»¶
        set_current_operation("æ‰«æè§†é¢‘æ–‡ä»¶")
        if _error_handler.verbose:
            print("ğŸ” å¼€å§‹æ‰«æè§†é¢‘æ–‡ä»¶...")
        
        video_files = scanner.scan_directory(str(directory), recursive=args.recursive)
        check_interruption()
        
        if not video_files:
            print(f"â„¹ï¸  åœ¨ç›®å½• {directory} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            if _error_handler.verbose:
                print("ğŸ’¡ å»ºè®®:")
                print(f"  â€¢ æ£€æŸ¥ç›®å½•è·¯å¾„æ˜¯å¦æ­£ç¡®: {directory}")
                print(f"  â€¢ æ£€æŸ¥æ–‡ä»¶æ‰©å±•åè¿‡æ»¤: {args.extensions}")
                print(f"  â€¢ ç¡®è®¤ç›®å½•ä¸­åŒ…å«è§†é¢‘æ–‡ä»¶")
            return 0
        
        print(f"âœ… æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå¼€å§‹æå–ä¿¡æ¯...")
        if _error_handler.verbose:
            print("ğŸ“‹ æ–‡ä»¶åˆ—è¡¨:")
            for i, file_path in enumerate(video_files[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i}. {os.path.basename(file_path)}")
            if len(video_files) > 5:
                print(f"  ... è¿˜æœ‰ {len(video_files) - 5} ä¸ªæ–‡ä»¶")
        
        # é¢„è§ˆæ¨¡å¼
        if args.dry_run:
            print("\nğŸ” é¢„è§ˆæ¨¡å¼ - ä¸å†™å…¥æ–‡ä»¶:")
            for i, video_file in enumerate(video_files[:5], 1):
                file_path = Path(video_file)
                try:
                    file_size = file_path.stat().st_size
                    print(f"  {i}. {file_path.name} ({format_file_size(file_size)})")
                except OSError as e:
                    print(f"  {i}. {file_path.name} (æ— æ³•è·å–æ–‡ä»¶å¤§å°: {e})")
            if len(video_files) > 5:
                print(f"  ... è¿˜æœ‰ {len(video_files) - 5} ä¸ªæ–‡ä»¶")
            return 0
        
        # æå–è§†é¢‘ä¿¡æ¯
        set_current_operation("æå–è§†é¢‘å…ƒæ•°æ®")
        video_infos = []
        failed_files = []
        
        if _error_handler.verbose:
            print("\nğŸ”„ å¼€å§‹æå–è§†é¢‘å…ƒæ•°æ®...")
        
        for i, video_file in enumerate(video_files, 1):
            check_interruption()
            
            if _error_handler.debug_mode:
                print(f"ğŸ” å¤„ç† {i}/{len(video_files)}: {video_file}")
            else:
                print(f"ğŸ“¹ å¤„ç† {i}/{len(video_files)}: {Path(video_file).name}")
            
            try:
                video_info = metadata_extractor.extract_metadata(video_file)
                # æ·»åŠ æ ‡ç­¾å’Œé€»è¾‘è·¯å¾„ä¿¡æ¯
                if args.tags:
                    # ä½¿ç”¨åˆ†å·åˆ†éš”æ ‡ç­¾
                    video_info.tags = [tag.strip() for tag in args.tags.split(';')]
                else:
                    # å¦‚æœæ²¡æœ‰è®¾ç½®tagsï¼Œä½¿ç”¨ç›®å½•åä½œä¸ºé»˜è®¤å€¼
                    directory_name = Path(video_file).parent.name
                    video_info.tags = [directory_name] if directory_name else []
                if args.path:
                    video_info.logical_path = args.path
                video_infos.append(video_info)
                
                if _error_handler.verbose:
                    print(f"  âœ… æˆåŠŸæå–å…ƒæ•°æ®")
                    print(f"     â€¢ åˆ†è¾¨ç‡: {getattr(video_info, 'width', 'N/A')}x{getattr(video_info, 'height', 'N/A')}")
                    print(f"     â€¢ æ—¶é•¿: {format_duration(getattr(video_info, 'duration', 0))}")
                    print(f"     â€¢ æ–‡ä»¶å¤§å°: {format_file_size(getattr(video_info, 'file_size', 0))}")
                    
            except FileNotFoundError:
                _error_handler.handle_file_not_found(video_file, "è§†é¢‘æ–‡ä»¶")
                failed_files.append(video_file)
                continue
            except PermissionError:
                _error_handler.handle_permission_error(video_file, "è¯»å–")
                failed_files.append(video_file)
                continue
            except Exception as e:
                _error_handler.handle_metadata_error(video_file, str(e))
                failed_files.append(video_file)
                continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶
        if not video_infos:
            print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è§†é¢‘æ–‡ä»¶")
            if failed_files:
                print(f"å¤±è´¥çš„æ–‡ä»¶æ•°é‡: {len(failed_files)}")
            return 1
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼å†™å…¥æ–‡ä»¶
        set_current_operation("å†™å…¥æ–‡ä»¶")
        if output_format == 'sqlite':
            # éªŒè¯æ•°æ®åº“è·¯å¾„
            if not _error_handler.validate_database_path(output_file):
                return 1
            
            try:
                # ç›´æ¥å†™å…¥SQLiteæ•°æ®åº“
                storage = SQLiteStorage(output_file)
                success_count = 0
                db_failed_count = 0
                
                for video_info in video_infos:
                    check_interruption()
                    try:
                        video_id = storage.insert_video_info(video_info)
                        if video_id:
                            success_count += 1
                        else:
                            db_failed_count += 1
                    except Exception as e:
                        _error_handler.handle_database_error(f"å†™å…¥è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}", output_file, "æ’å…¥è®°å½•")
                        db_failed_count += 1
                
                # æ·»åŠ æ‰«æå†å²è®°å½•
                try:
                    if args.tags:
                        # ä½¿ç”¨åˆ†å·åˆ†éš”æ ‡ç­¾
                        tags_list = [tag.strip() for tag in args.tags.split(';')]
                    else:
                        tags_list = None
                    storage.add_scan_history(
                        scan_path=args.directory,
                        files_found=len(video_files),
                        files_processed=success_count,
                        tags=tags_list,
                        logical_path=args.path
                    )
                except Exception as e:
                    _error_handler.handle_database_error(f"è®°å½•æ‰«æå†å²å¤±è´¥: {e}", output_file, "æ·»åŠ å†å²è®°å½•")
                
                storage.close()
                
                print(f"\nâœ… æ‰«æå®Œæˆ!")
                print(f"ğŸ“Š å¤„ç†ç»“æœ:")
                print(f"  â€¢ å‘ç°æ–‡ä»¶: {len(video_files)}")
                print(f"  â€¢ æˆåŠŸå¤„ç†: {len(video_infos)}")
                print(f"  â€¢ å†™å…¥æ•°æ®åº“: {success_count}")
                if failed_files:
                    print(f"  â€¢ å¤„ç†å¤±è´¥: {len(failed_files)}")
                if db_failed_count > 0:
                    print(f"  â€¢ æ•°æ®åº“å†™å…¥å¤±è´¥: {db_failed_count}")
                print(f"ğŸ“ SQLiteæ•°æ®åº“: {output_file}")
                
            except Exception as e:
                _error_handler.handle_database_error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}", output_file)
                return 1
        else:
            try:
                # å†™å…¥CSVæ–‡ä»¶ï¼ˆä¸´æ—¶æ–‡ä»¶æˆ–æœ€ç»ˆæ–‡ä»¶ï¼‰
                csv_writer = CSVWriter()
                csv_writer.write_video_infos(video_infos, output_file)
                
                print(f"\nâœ… æ‰«æå®Œæˆ!")
                print(f"ğŸ“Š å¤„ç†ç»“æœ:")
                print(f"  â€¢ å‘ç°æ–‡ä»¶: {len(video_files)}")
                print(f"  â€¢ æˆåŠŸå¤„ç†: {len(video_infos)}")
                if failed_files:
                    print(f"  â€¢ å¤„ç†å¤±è´¥: {len(failed_files)}")
                print(f"ğŸ“ CSVæ–‡ä»¶: {output_file}")
                
                # å¦‚æœæ˜¯ä¸´æ—¶æ–‡ä»¶ï¼Œæç¤ºåˆå¹¶æ“ä½œ
                if not args.output and not args.temp_file:
                    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
                    print(f"  è¯·ç”¨Excel/Numbers/æµè§ˆå™¨æŸ¥çœ‹ä¸´æ—¶æ–‡ä»¶å†…å®¹ï¼Œç¡®è®¤æ— è¯¯åæ‰§è¡Œåˆå¹¶æ“ä½œã€‚")
                    print(f"\nğŸ“ åˆå¹¶å‘½ä»¤:")
                    print(f"  python -m tools.video_info_collector --merge {output_file}")
                    
            except Exception as e:
                _error_handler.handle_generic_error(e, "å†™å…¥CSVæ–‡ä»¶")
                return 1
        
        return 0
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_generic_error(e, "æ‰«ææ“ä½œ")
        return 1


def merge_command(args):
    """åˆå¹¶CSVæ–‡ä»¶åˆ°SQLiteæ•°æ®åº“"""
    global _error_handler
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    set_current_operation("åˆå¹¶CSVæ–‡ä»¶åˆ°æ•°æ®åº“")
    
    csv_file = args.csv_file
    
    # éªŒè¯CSVæ–‡ä»¶
    if not _error_handler.validate_file_path(csv_file, "CSVæ–‡ä»¶", must_exist=True):
        return 1
    
    # éªŒè¯æ•°æ®åº“è·¯å¾„ï¼ˆåˆå¹¶æ“ä½œå…è®¸æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºï¼‰
    if not _error_handler.validate_database_path(args.database, must_exist=False):
        return 1
    
    print(f"ğŸ“ æ­£åœ¨åˆå¹¶ä¸´æ—¶æ–‡ä»¶: {csv_file}")
    print(f"ğŸ—„ï¸  ç›®æ ‡æ•°æ®åº“: {args.database}")
    print(f"ğŸ”„ é‡å¤ç­–ç•¥: {args.duplicate_strategy}")
    
    try:
        set_current_operation("è¿æ¥æ•°æ®åº“")
        # åˆå§‹åŒ–å­˜å‚¨
        storage = SQLiteStorage(args.database)
        
        set_current_operation("ç”ŸæˆCSVæ–‡ä»¶æŒ‡çº¹")
        # ç”ŸæˆCSVæ–‡ä»¶æŒ‡çº¹
        print("æ­£åœ¨ç”ŸæˆCSVæ–‡ä»¶æŒ‡çº¹...")
        csv_fingerprint = storage.get_csv_fingerprint(csv_file)
        print(f"CSVæ–‡ä»¶æŒ‡çº¹: {csv_fingerprint[:16]}...")
        check_interruption()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå¹¶è¿‡
        if storage.check_csv_already_merged(csv_fingerprint):
            print(f"âš ï¸  è­¦å‘Š: è¯¥CSVæ–‡ä»¶å·²ç»è¢«åˆå¹¶è¿‡!")
            print(f"æ–‡ä»¶æŒ‡çº¹: {csv_fingerprint}")
            
            # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
            if hasattr(args, 'force') and args.force:
                print("ä½¿ç”¨ --force å‚æ•°ï¼Œå¼ºåˆ¶é‡æ–°åˆå¹¶...")
            else:
                try:
                    response = input("æ˜¯å¦è¦å¼ºåˆ¶é‡æ–°åˆå¹¶? (y/N): ").strip().lower()
                    if response not in ['y', 'yes']:
                        print("å–æ¶ˆåˆå¹¶æ“ä½œ")
                        storage.close()
                        return 0
                except (EOFError, KeyboardInterrupt):
                    print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
                    storage.close()
                    return 130
        
        set_current_operation("åˆ†æCSVæ–‡ä»¶")
        # ä»æ–‡ä»¶åæå–æ‰«æä¿¡æ¯
        scan_info = storage.extract_scan_info_from_csv_filename(csv_file)
        print(f"æ¨æ–­çš„åŸå§‹æ‰«æè·¯å¾„: {scan_info['original_scan_path']}")
        print(f"æ‰«ææ—¶é—´æˆ³: {scan_info['timestamp']}")
        
        # ç»Ÿè®¡CSVæ–‡ä»¶ä¸­çš„è®°å½•æ•°
        import csv as csv_module
        with open(csv_file, 'r', encoding='utf-8') as f:
            csv_reader = csv_module.reader(f)
            next(csv_reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
            total_records = sum(1 for _ in csv_reader)
        
        print(f"CSVæ–‡ä»¶åŒ…å« {total_records} æ¡è®°å½•")
        check_interruption()
        
        # æ‰§è¡Œæ™ºèƒ½åˆå¹¶æ“ä½œ
        set_current_operation("æ™ºèƒ½åˆå¹¶CSVæ•°æ®")
        print("å¼€å§‹æ™ºèƒ½åˆå¹¶æ•°æ®...")
        
        # å¯¼å…¥SmartMergeManager
        from .smart_merge_manager import SmartMergeManager
        
        # ä»CSVæ–‡ä»¶åŠ è½½è§†é¢‘ä¿¡æ¯
        new_videos = storage.load_videos_from_csv(args.csv_file)
        if not new_videos:
            print("âŒ CSVæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘æ•°æ®")
            storage.close()
            return 1
        
        # è·å–ç°æœ‰è§†é¢‘æ•°æ®
        existing_videos = storage.get_all_video_infos()
        
        # åˆ›å»ºæ™ºèƒ½åˆå¹¶ç®¡ç†å™¨
        merge_manager = SmartMergeManager(storage)
        
        # åˆ†æåˆå¹¶å€™é€‰é¡¹
        merge_results = merge_manager.analyze_merge_candidates(new_videos, existing_videos)
        
        # è®°å½•åˆå¹¶å†å²ï¼ˆåˆ›å»ºscanè®°å½•ï¼‰
        set_current_operation("è®°å½•åˆå¹¶å†å²")
        history_id = storage.add_csv_merge_history(
            csv_file_path=csv_file,
            files_found=total_records,
            files_processed=0,  # å°†åœ¨æ‰§è¡Œåæ›´æ–°
            csv_fingerprint=csv_fingerprint,
            original_scan_path=scan_info['original_scan_path'],
            tags=None,  # CSVåˆå¹¶æ“ä½œé€šå¸¸ä¸æ¶‰åŠç‰¹å®šæ ‡ç­¾
            logical_path=scan_info['original_scan_path']
        )
        
        # æ‰§è¡Œåˆå¹¶è®¡åˆ’
        set_current_operation("æ‰§è¡Œåˆå¹¶è®¡åˆ’")
        merge_stats = merge_manager.execute_merge_plan(merge_results, history_id)
        success_count = merge_stats['inserted'] + merge_stats['updated']
        
        # ä¸ºè·³è¿‡çš„é‡å¤è§†é¢‘è®°å½•merge_historyäº‹ä»¶
        total_actions = sum(len(actions) for actions in merge_results.values())
        skipped_count = len(new_videos) - total_actions
        
        if skipped_count > 0:
            # ä¸ºè·³è¿‡çš„è§†é¢‘è®°å½•mergeäº‹ä»¶
            processed_videos = set()
            for action_list in merge_results.values():
                for action in action_list:
                    processed_videos.add(action.video_info.file_path)
            
            for new_video in new_videos:
                if new_video.file_path not in processed_videos:
                    # è¿™æ˜¯ä¸€ä¸ªè¢«è·³è¿‡çš„é‡å¤è§†é¢‘ï¼Œè®°å½•mergeäº‹ä»¶
                    storage.add_merge_event(
                        'skip_duplicate',
                        None, 
                        new_video.file_path,
                        history_id
                    )
        
        # æ›´æ–°åˆå¹¶å†å²è®°å½•çš„å¤„ç†æ•°é‡
        storage.update_csv_merge_history_processed_count(history_id, success_count)
        
        check_interruption()
        
        storage.close()
        
        print(f"\nâœ… åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"  â€¢ CSVè®°å½•æ•°: {total_records}")
        print(f"  â€¢ æˆåŠŸå¯¼å…¥: {success_count}")
        if success_count < total_records:
            print(f"  â€¢ è·³è¿‡è®°å½•: {total_records - success_count} (å¯èƒ½æ˜¯é‡å¤è®°å½•)")
        print(f"ğŸ“ æ•°æ®åº“æ–‡ä»¶: {args.database}")
        print(f"ğŸ“ åˆå¹¶å†å²è®°å½•ID: {history_id}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ åˆå¹¶æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_database_error(f"åˆå¹¶æ“ä½œå¤±è´¥: {e}", args.database, "å¯¼å…¥CSV")
        return 1


def export_command(args):
    """ä»SQLiteæ•°æ®åº“å¯¼å‡ºåˆ°CSV"""
    global _error_handler
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    set_current_operation("å¯¼å‡ºæ•°æ®åº“åˆ°CSV")
    
    # éªŒè¯æ•°æ®åº“è·¯å¾„ï¼ˆå¯¼å‡ºæ“ä½œéœ€è¦æ•°æ®åº“æ–‡ä»¶å­˜åœ¨ï¼‰
    if not _error_handler.validate_database_path(args.database, must_exist=True):
        return 1
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    if not args.output:
        default_paths = get_default_paths()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"exported_videos_{timestamp}.csv"
        args.output = str(Path(default_paths['csv_dir']) / output_filename)
    
    print(f"ğŸ—„ï¸  æ•°æ®åº“æ–‡ä»¶: {args.database}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    try:
        set_current_operation("è¿æ¥æ•°æ®åº“")
        # å†æ¬¡æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆé˜²æ­¢SQLiteè‡ªåŠ¨åˆ›å»ºç©ºæ•°æ®åº“ï¼‰
        if not os.path.exists(args.database):
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.database}")
            return 1
        storage = SQLiteStorage(args.database)
        
        set_current_operation("å¯¼å‡ºCSVæ–‡ä»¶")
        # ä½¿ç”¨SQLiteStorageçš„export_to_csvæ–¹æ³•
        success = storage.export_to_csv(args.output)
        check_interruption()
        
        if success:
            # è·å–è®°å½•æ•°é‡
            total_count = storage.get_total_count()
            storage.close()
            print(f"\nâœ… å¯¼å‡ºå®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»“æœ:")
            print(f"  â€¢ å¯¼å‡ºè®°å½•æ•°: {total_count}")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
            return 0
        else:
            storage.close()
            print("\nâŒ å¯¼å‡ºå¤±è´¥")
            return 1
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ å¯¼å‡ºæ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_database_error(f"å¯¼å‡ºæ“ä½œå¤±è´¥: {e}", args.database, "å¯¼å‡ºCSV")
        return 1


def export_simple_command(args):
    """ä»SQLiteæ•°æ®åº“ç®€åŒ–å¯¼å‡ºï¼ˆä»…åŒ…å«filenameã€filesizeã€logical_pathï¼‰"""
    global _error_handler
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    set_current_operation("å¯¼å‡ºç®€åŒ–è§†é¢‘ä¿¡æ¯")
    
    # éªŒè¯æ•°æ®åº“è·¯å¾„
    if not _error_handler.validate_database_path(args.database):
        return 1
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    if not args.output:
        default_paths = get_default_paths()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"simple_export_{timestamp}.txt"
        args.output = str(Path(default_paths['csv_dir']) / output_filename)
    
    print(f"ğŸ—„ï¸  æ•°æ®åº“æ–‡ä»¶: {args.database}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
    print("ğŸ“‹ è¾“å‡ºæ ¼å¼: filename filesize logical_path")
    
    try:
        set_current_operation("è¿æ¥æ•°æ®åº“")
        # å†æ¬¡æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆé˜²æ­¢SQLiteè‡ªåŠ¨åˆ›å»ºç©ºæ•°æ®åº“ï¼‰
        if not os.path.exists(args.database):
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.database}")
            return 1
        storage = SQLiteStorage(args.database)
        
        set_current_operation("å¯¼å‡ºç®€åŒ–ä¿¡æ¯")
        # ä½¿ç”¨SQLiteStorageçš„export_simple_formatæ–¹æ³•
        success = storage.export_simple_format(args.output)
        check_interruption()
        
        if success:
            # è·å–è®°å½•æ•°é‡
            total_count = storage.get_total_count()
            storage.close()
            print(f"\nâœ… ç®€åŒ–å¯¼å‡ºå®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»“æœ:")
            print(f"  â€¢ å¯¼å‡ºè®°å½•æ•°: {total_count}")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output}")
            return 0
        else:
            storage.close()
            print("\nâŒ ç®€åŒ–å¯¼å‡ºå¤±è´¥")
            return 1
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ å¯¼å‡ºæ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_database_error(f"ç®€åŒ–å¯¼å‡ºå¤±è´¥: {e}", args.database, "å¯¼å‡ºç®€åŒ–ä¿¡æ¯")
        return 1


def search_video_code_command(args):
    """è§†é¢‘codeæŸ¥è¯¢å‘½ä»¤"""
    global _error_handler
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    set_current_operation("è§†é¢‘codeæŸ¥è¯¢")
    
    # è§£æè¾“å…¥çš„video_codesï¼Œæ”¯æŒç©ºæ ¼å’Œé€—å·åˆ†éš”
    video_codes_input = args.search_video_codes.strip()
    if not video_codes_input:
        print("âŒ é”™è¯¯: è¯·æä¾›è¦æŸ¥è¯¢çš„è§†é¢‘code")
        return 1
    
    # åˆ†å‰²video_codesï¼Œæ”¯æŒé€—å·å’Œç©ºæ ¼åˆ†éš”
    import re
    video_codes = re.split(r'[,\s]+', video_codes_input)
    # å»é™¤ç©ºå­—ç¬¦ä¸²å’Œå‰åç©ºæ ¼
    video_codes = [video_code.strip() for video_code in video_codes if video_code.strip()]
    
    if not video_codes:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘code")
        return 1
    
    print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢è§†é¢‘code: {', '.join(video_codes)}")
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not os.path.exists(args.database):
            print(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.database}")
            print("ğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œæ‰«æå‘½ä»¤ç”Ÿæˆæ•°æ®åº“ï¼Œæˆ–ä½¿ç”¨ --init-db åˆå§‹åŒ–æ•°æ®åº“")
            return 1
        
        storage = SQLiteStorage(args.database)
        
        # æŸ¥è¯¢è§†é¢‘ä¿¡æ¯
        results = storage.search_videos_by_video_codes(video_codes)
        
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è§†é¢‘")
            print(f"ğŸ” æŸ¥è¯¢çš„video_codes: {', '.join(video_codes)}")
            storage.close()
            return 0
        
        # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ
        print(f"\nâœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…çš„è§†é¢‘:")
        print("-" * 80)
        print(f"{'è§†é¢‘Code':<20} {'æ–‡ä»¶å¤§å°':<15} {'é€»è¾‘è·¯å¾„':<30}")
        print("-" * 80)
        
        for result in results:
            video_code = result['video_code']
            file_size = format_file_size(result['file_size']) if result['file_size'] else 'N/A'
            logical_path = result['logical_path'] or 'N/A'
            
            print(f"{video_code:<20} {file_size:<15} {logical_path:<30}")
        
        storage.close()
        return 0
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ è§†é¢‘codeæŸ¥è¯¢è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_generic_error(f"è§†é¢‘codeæŸ¥è¯¢å¤±è´¥: {e}", "è§†é¢‘codeæŸ¥è¯¢")
        return 1


def init_db_command(args):
    """åˆå§‹åŒ–/é‡ç½®æ•°æ®åº“"""
    global _error_handler
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    set_current_operation("åˆå§‹åŒ–æ•°æ®åº“")
    
    db_path = args.database
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(db_path):
        # è¯¢é—®ç”¨æˆ·ç¡®è®¤
        print(f"âš ï¸  æ•°æ®åº“æ–‡ä»¶å·²å­˜åœ¨: {db_path}")
        print("æ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®ï¼")
        
        # åœ¨éäº¤äº’æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ç”¨æˆ·æ˜ç¡®ç¡®è®¤
        try:
            confirm = input("ç¡®è®¤è¦é‡ç½®æ•°æ®åº“å—ï¼Ÿ(è¾“å…¥ 'yes' ç¡®è®¤): ").strip().lower()
            if confirm != 'yes':
                print("æ“ä½œå·²å–æ¶ˆ")
                return 0
        except (EOFError, KeyboardInterrupt):
            print("\næ“ä½œå·²å–æ¶ˆ")
            return 0
        
        # åˆ é™¤ç°æœ‰æ•°æ®åº“æ–‡ä»¶
        try:
            Path(db_path).unlink()
            print(f"å·²åˆ é™¤ç°æœ‰æ•°æ®åº“æ–‡ä»¶: {db_path}")
        except Exception as e:
            print(f"åˆ é™¤æ•°æ®åº“æ–‡ä»¶å¤±è´¥: {e}")
            return 1
    
    # åˆ›å»ºæ–°çš„æ•°æ®åº“
    try:
        print(f"ğŸ—„ï¸  æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“: {db_path}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        db_dir = os.path.dirname(db_path)
        if db_dir:
            os.makedirs(db_dir, exist_ok=True)
        
        set_current_operation("åˆ›å»ºæ•°æ®åº“ç»“æ„")
        # åˆ›å»ºæ–°çš„SQLiteStorageå®ä¾‹ï¼Œè¿™ä¼šè‡ªåŠ¨åˆ›å»ºè¡¨ç»“æ„
        storage = SQLiteStorage(db_path)
        
        # éªŒè¯æ•°æ®åº“åˆ›å»ºæˆåŠŸ
        validation_results = storage.validate_database_structure()
        total_count = storage.get_total_count()
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¡¨éƒ½åˆ›å»ºæˆåŠŸ
        failed_tables = [table for table, created in validation_results.items() if not created]
        if failed_tables:
            storage.close()
            print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼ä»¥ä¸‹è¡¨æœªèƒ½åˆ›å»º: {', '.join(failed_tables)}")
            return 1
        
        storage.close()
        check_interruption()
        
        print(f"\nâœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ!")
        print(f"ğŸ“ æ•°æ®åº“æ–‡ä»¶: {db_path}")
        print(f"ğŸ“‹ å·²åˆ›å»ºçš„è¡¨:")
        for table_name, created in validation_results.items():
            status = "âœ…" if created else "âŒ"
            table_descriptions = {
                'video_info': 'è§†é¢‘å…ƒæ•°æ®è¡¨',
                'video_tags': 'è§†é¢‘æ ‡ç­¾è¡¨',
                'scan_history': 'æ‰«æå†å²è¡¨',
                'video_master_list': 'è§†é¢‘ä¸»åˆ—è¡¨è¡¨',
                'merge_history': 'åˆå¹¶å†å²è¡¨'
            }
            description = table_descriptions.get(table_name, table_name)
            print(f"  {status} {table_name} - {description}")
        print(f"ğŸ“Š å½“å‰è®°å½•æ•°: {total_count}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ•°æ®åº“åˆå§‹åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_database_error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}", db_path, "åˆå§‹åŒ–")
        return 1


def stats_command(args):
    """å¤„ç†æ•°æ®ç»Ÿè®¡å‘½ä»¤"""
    try:
        setup_signal_handlers()
        set_current_operation("æ•°æ®ç»Ÿè®¡")
        
        db_path = args.database
        
        # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(db_path):
            print(f"âŒ é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            print("ğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œæ‰«æå‘½ä»¤ç”Ÿæˆæ•°æ®ï¼Œæˆ–ä½¿ç”¨ --init-db åˆå§‹åŒ–æ•°æ®åº“")
            return 1
        
        # è¿æ¥æ•°æ®åº“
        storage = SQLiteStorage(db_path)
        check_interruption()
        
        if args.group_by:
            # åˆ†ç»„ç»Ÿè®¡
            if args.group_by == 'tags':
                print("ğŸ“Š æŒ‰æ ‡ç­¾åˆ†ç»„ç»Ÿè®¡:")
                print("=" * 50)
                stats = storage.get_statistics_by_tags()
                if stats:
                    for stat in stats:
                        tag = stat['tag']
                        count = stat['count']
                        print(f"{tag}: {count} ä¸ªè§†é¢‘")
                else:
                    print("æš‚æ— æ•°æ®")
                    
            elif args.group_by == 'resolution':
                print("ğŸ“Š æŒ‰åˆ†è¾¨ç‡åˆ†ç»„ç»Ÿè®¡:")
                print("=" * 50)
                stats = storage.get_statistics_by_resolution()
                if stats:
                    for stat in stats:
                        resolution = stat['resolution']
                        count = stat['count']
                        print(f"{resolution}: {count} ä¸ªè§†é¢‘")
                else:
                    print("æš‚æ— æ•°æ®")
                    
            elif args.group_by == 'duration':
                print("ğŸ“Š æŒ‰æ—¶é•¿åˆ†ç»„ç»Ÿè®¡:")
                print("=" * 50)
                stats = storage.get_statistics_by_duration()
                if stats:
                    for stat in stats:
                        duration_range = stat['duration_range']
                        count = stat['count']
                        print(f"{duration_range}: {count} ä¸ªè§†é¢‘")
                else:
                    print("æš‚æ— æ•°æ®")
        else:
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
            print("=" * 50)
            
            enhanced_stats = storage.get_enhanced_statistics()
            
            # åŸºæœ¬ä¿¡æ¯
            print(f"ğŸ“¹ æ€»è§†é¢‘æ•°: {enhanced_stats['total_videos']}")
            print(f"ğŸ”„ åŒåè§†é¢‘ç»„æ•°: {enhanced_stats['duplicate_video_groups']}")
            print(f"ğŸ“ å”¯ä¸€è§†é¢‘æ•°: {enhanced_stats['unique_videos']}")
            
            # å®¹é‡ä¿¡æ¯
            total_size_gb = enhanced_stats['total_size'] / (1024**3) if enhanced_stats['total_size'] else 0
            avg_size_mb = enhanced_stats['avg_file_size'] / (1024**2) if enhanced_stats['avg_file_size'] else 0
            print(f"ğŸ’¾ æ€»å®¹é‡: {total_size_gb:.2f} GB")
            print(f"ğŸ“ å¹³å‡æ–‡ä»¶å¤§å°: {avg_size_mb:.2f} MB")
            
            # æ—¶é•¿ä¿¡æ¯
            total_hours = enhanced_stats['total_duration'] / 3600 if enhanced_stats['total_duration'] else 0
            avg_minutes = enhanced_stats['avg_duration'] / 60 if enhanced_stats['avg_duration'] else 0
            print(f"â±ï¸ æ€»æ—¶é•¿: {total_hours:.2f} å°æ—¶")
            print(f"â° å¹³å‡æ—¶é•¿: {avg_minutes:.2f} åˆ†é’Ÿ")
            
            # ç¼–ç ä¿¡æ¯
            if enhanced_stats['most_common_codec']:
                print(f"ğŸ¬ æœ€å¸¸è§ç¼–ç : {enhanced_stats['most_common_codec']}")
            
            # åˆ†è¾¨ç‡åˆ†å¸ƒ
            if enhanced_stats['resolution_distribution']:
                print("\nğŸ“º åˆ†è¾¨ç‡åˆ†å¸ƒ:")
                for resolution, count in enhanced_stats['resolution_distribution'].items():
                    print(f"  {resolution}: {count} ä¸ªè§†é¢‘")
        
        storage.close()
        check_interruption()
        
        return 0
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç»Ÿè®¡æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        _error_handler.handle_database_error(f"ç»Ÿè®¡æ“ä½œå¤±è´¥: {e}", args.database, "ç»Ÿè®¡")
        return 1


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    # è·å–é»˜è®¤è·¯å¾„é…ç½®
    default_paths = get_default_paths()
    
    parser = argparse.ArgumentParser(
        description="Video Info Collector - è§†é¢‘æ–‡ä»¶ä¿¡æ¯æ”¶é›†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ‰«æç›®å½•ç”Ÿæˆä¸´æ—¶æ–‡ä»¶
  python -m tools.video_info_collector /path/to/videos
  
  # æŒ‡å®šæ ‡ç­¾å’Œè·¯å¾„
  python -m tools.video_info_collector /path/to/videos --tags "åŠ¨ä½œç‰‡,é«˜æ¸…" --path "ç”µå½±/åŠ¨ä½œç‰‡/2024"
  
  # æŒ‡å®šè¾“å‡ºæ ¼å¼
  python -m tools.video_info_collector /path/to/videos --output-format csv  # é»˜è®¤
  python -m tools.video_info_collector /path/to/videos --output-format sqlite  # ç›´æ¥è¾“å‡ºSQLite
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python -m tools.video_info_collector /path/to/videos --output /path/to/output.csv
  python -m tools.video_info_collector /path/to/videos --output /path/to/output.db
  
  # åˆå¹¶ä¸´æ—¶æ–‡ä»¶åˆ°æ•°æ®åº“
  python -m tools.video_info_collector --merge output/video_info_collector/csv/temp_video_info_20240120_154500.csv
  
  # ä»æ•°æ®åº“å¯¼å‡º
  python -m tools.video_info_collector --export output/video_info_collector/database/video_database.db --output output/video_info_collector/csv/exported_data.csv
  
  # ä»æ•°æ®åº“ç®€åŒ–å¯¼å‡ºï¼ˆä»…åŒ…å«filenameã€filesizeã€logical_pathï¼‰
  python -m tools.video_info_collector --export-simple output/video_info_collector/database/video_database.db --output simple_export.txt
  
  # æ ¹æ®è§†é¢‘codeæŸ¥è¯¢
  python -m tools.video_info_collector --search-video-code "ABC-123"
  python -m tools.video_info_collector --search-video-code "ABC-123,DEF-456"
  python -m tools.video_info_collector --search-video-code "ABC-123 DEF-456"
  
  # æ•°æ®ç»Ÿè®¡
  python -m tools.video_info_collector --stats  # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
  python -m tools.video_info_collector --stats --group-by tags  # æŒ‰æ ‡ç­¾åˆ†ç»„ç»Ÿè®¡
  python -m tools.video_info_collector --stats --group-by resolution  # æŒ‰åˆ†è¾¨ç‡åˆ†ç»„ç»Ÿè®¡
  python -m tools.video_info_collector --stats --group-by duration  # æŒ‰æ—¶é•¿åˆ†ç»„ç»Ÿè®¡
  
  # åˆå§‹åŒ–/é‡ç½®æ•°æ®åº“
  python -m tools.video_info_collector --init-db
  python -m tools.video_info_collector --init-db --database /path/to/custom.db
        """
    )
    
    # å…¨å±€å‚æ•°
    parser.add_argument('--database', default=default_paths['default_database'],
                       help=f'SQLiteæ•°æ®åº“æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_paths["default_database"]})')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='å¯ç”¨è¯¦ç»†è¾“å‡ºæ¨¡å¼ï¼Œæ˜¾ç¤ºæ›´å¤šæ“ä½œä¿¡æ¯')
    
    # äº’æ–¥ç»„ï¼šæ‰«æã€åˆå¹¶æˆ–å¯¼å‡º
    group = parser.add_mutually_exclusive_group(required=False)
    
    # åˆå¹¶æ“ä½œ
    group.add_argument('--merge', dest='csv_file', metavar='CSV_FILE',
                      help='åˆå¹¶ä¸´æ—¶CSVæ–‡ä»¶åˆ°æ•°æ®åº“')
    
    # å¯¼å‡ºæ“ä½œ
    group.add_argument('--export', dest='export_db', metavar='DATABASE',
                      help='ä»æ•°æ®åº“å¯¼å‡ºåˆ°CSVæ–‡ä»¶')
    
    # ç®€åŒ–å¯¼å‡ºæ“ä½œ
    group.add_argument('--export-simple', dest='export_simple_db', metavar='DATABASE',
                      help='ä»æ•°æ®åº“ç®€åŒ–å¯¼å‡ºï¼ˆä»…åŒ…å«filenameã€filesizeã€logical_pathï¼‰')
    
    # æ•°æ®åº“åˆå§‹åŒ–æ“ä½œ
    group.add_argument('--init-db', action='store_true',
                      help='åˆå§‹åŒ–/é‡ç½®æ•°æ®åº“ï¼ˆæ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼‰')
    
    # è§†é¢‘codeæŸ¥è¯¢æ“ä½œ
    group.add_argument('--search-video-code', dest='search_video_codes', metavar='VIDEO_CODES',
                      help='æ ¹æ®è§†é¢‘codeæŸ¥è¯¢ï¼ˆæ”¯æŒå¤šä¸ªvideo_codeï¼Œç”¨ç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼‰')
    
    # æ•°æ®ç»Ÿè®¡æ“ä½œ
    group.add_argument('--stats', action='store_true',
                      help='æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯')
    
    # æ‰«æç›®å½•ï¼ˆä½ç½®å‚æ•°ï¼‰
    parser.add_argument('directory', nargs='?',
                       help='è¦æ‰«æçš„ç›®å½•è·¯å¾„')
    
    # æ‰«æå‚æ•°
    parser.add_argument('--tags',
                       help='ä¸ºæ‰€æœ‰æ–‡ä»¶æ·»åŠ çš„æ ‡ç­¾ï¼ˆåˆ†å·åˆ†éš”ï¼Œå¦‚ï¼šåŠ¨ä½œç‰‡;é«˜æ¸…;2024ï¼‰')
    parser.add_argument('--path',
                       help='é€»è¾‘è·¯å¾„ä¿¡æ¯')
    parser.add_argument('--temp-file',
                       help='ä¸´æ—¶æ”¶é›†æ–‡ä»¶å')
    parser.add_argument('--dry-run', action='store_true',
                       help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ–‡ä»¶')
    parser.add_argument('--recursive', action='store_true', default=True,
                       help='é€’å½’æ‰«æå­ç›®å½• (é»˜è®¤: True)')
    parser.add_argument('--extensions', 
                       default='.mp4,.mkv,.avi,.mov,.wmv,.flv',
                       help='è§†é¢‘æ–‡ä»¶æ‰©å±•åè¿‡æ»¤')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-format', choices=['csv', 'sqlite'], default='csv',
                       help='è¾“å‡ºæ ¼å¼ï¼šcsv/sqlite (é»˜è®¤: csv)')
    parser.add_argument('--output',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # åˆå¹¶å‚æ•°
    parser.add_argument('--duplicate-strategy', 
                       choices=['skip', 'update', 'append'], 
                       default='skip',
                       help='é‡å¤é¡¹å¤„ç†ç­–ç•¥ (é»˜è®¤: skip)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°åˆå¹¶å·²ç»åˆå¹¶è¿‡çš„CSVæ–‡ä»¶')
    
    # å¯¼å‡ºå‚æ•°
    parser.add_argument('--format', choices=['csv', 'json'], default='csv',
                       help='å¯¼å‡ºæ ¼å¼ (é»˜è®¤: csv)')
    
    # ç»Ÿè®¡å‚æ•°
    parser.add_argument('--group-by', choices=['tags', 'resolution', 'duration'], 
                       help='åˆ†ç»„ç»Ÿè®¡ç»´åº¦ï¼štags(æ ‡ç­¾)ã€resolution(åˆ†è¾¨ç‡)ã€duration(æ—¶é•¿)')
    
    return parser


def cli_main(argv=None):
    """CLIä¸»å‡½æ•°"""
    global _error_handler
    
    parser = create_parser()
    args = parser.parse_args(argv)
    
    # ç¡®ä¿é”™è¯¯å¤„ç†å™¨å·²åˆå§‹åŒ–ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if _error_handler is None:
        _error_handler = create_error_handler()
    
    # æ ¹æ®æ“ä½œç±»å‹è°ƒç”¨ç›¸åº”å‡½æ•°
    if args.csv_file:
        # åˆå¹¶æ“ä½œ
        return merge_command(args)
    elif args.export_db:
        # å¯¼å‡ºæ“ä½œ
        if not args.output:
            print("é”™è¯¯: å¯¼å‡ºæ“ä½œéœ€è¦æŒ‡å®š --output å‚æ•°")
            return 1
        # è®¾ç½®æ•°æ®åº“è·¯å¾„
        args.database = args.export_db
        return export_command(args)
    elif args.export_simple_db:
        # ç®€åŒ–å¯¼å‡ºæ“ä½œ
        # è®¾ç½®æ•°æ®åº“è·¯å¾„
        args.database = args.export_simple_db
        return export_simple_command(args)
    elif args.init_db:
        # æ•°æ®åº“åˆå§‹åŒ–æ“ä½œ
        return init_db_command(args)
    elif args.search_video_codes:
        # è§†é¢‘codeæŸ¥è¯¢æ“ä½œ
        return search_video_code_command(args)
    elif args.stats:
        # æ•°æ®ç»Ÿè®¡æ“ä½œ
        return stats_command(args)
    elif args.directory:
        # æ‰«ææ“ä½œ
        return scan_command(args)
    else:
        # å¦‚æœæ²¡æœ‰æä¾›ä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
        parser.print_help()
        return 1


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    global _error_handler
    
    try:
        # é¢„è§£æå‚æ•°ä»¥è·å–è°ƒè¯•æ¨¡å¼è®¾ç½®
        parser = create_parser()
        args, _ = parser.parse_known_args()
        
        # åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
        _error_handler = create_error_handler(debug_mode=args.debug, verbose=args.verbose)
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        setup_signal_handlers()
        
        # è®¾ç½®æ—¥å¿—è®°å½•
        setup_logging(debug_mode=args.debug)
        
        if args.verbose:
            print("ğŸ”§ è¯¦ç»†æ¨¡å¼å·²å¯ç”¨")
        if args.debug:
            print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
        
        exit_code = cli_main()
        sys.exit(exit_code)
        
    except SystemExit as e:
        return e.code
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        if _error_handler:
            _error_handler.handle_generic_error(f"ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}", "ç¨‹åºå¯åŠ¨")
        else:
            print(f"âŒ ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        return 1


if __name__ == '__main__':
    main()